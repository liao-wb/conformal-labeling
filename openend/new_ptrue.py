from typing import List, Dict, Any, Tuple
from vllm import LLM, SamplingParams
import numpy as np
import json
import re
from utils import save_result
from vllm.sampling_params import GuidedDecodingParams

class PTrueEvaluator:
    def __init__(self, args):
        """初始化 vLLM 模型"""
        self.llm = LLM(
            model=f"/mnt/sharedata/ssd_large/common/LLMs/{args.model}",
            trust_remote_code=True,
            tensor_parallel_size=args.tensor_parallel_size,
            gpu_memory_utilization=0.7,
            max_model_len=args.max_model_len,
        )
        self.tokenizer = self.llm.get_tokenizer()

        # ----- 选项 token（支持 A/B/C/D 与 A、B、C、D） -----
        # self.option_token_ids = {}
        # for ch in "ABCD":
        #     # 英文
        #     self.option_token_ids[ch] = self.tokenizer.convert_tokens_to_ids(ch)
        #     # 中文全角
        #     self.option_token_ids[f"{ch}、"] = self.tokenizer.convert_tokens_to_ids(f"{ch}、")
        # # 兼容可能的空格/逗号
        # for ch in "ABCD":
        #     self.option_token_ids[f"{ch} "] = self.tokenizer.convert_tokens_to_ids(f"{ch} ")
        #     self.option_token_ids[f"{ch},"] = self.tokenizer.convert_tokens_to_ids(f"{ch},")
        self.option_token_ids = {   'A': self.tokenizer.convert_tokens_to_ids("A"),
    'B': self.tokenizer.convert_tokens_to_ids("B"), 
    'C': self.tokenizer.convert_tokens_to_ids("C"),
    'D': self.tokenizer.convert_tokens_to_ids("D"),
    'E': self.tokenizer.convert_tokens_to_ids("E"),
    }
        # P(True) 仍然使用 A / B
        self.yes_token_id = self.tokenizer.convert_tokens_to_ids("A")
        self.no_token_id  = self.tokenizer.convert_tokens_to_ids("B")
        self.args = args

        print(f"Yes token ID: {self.yes_token_id}, No token ID: {self.no_token_id}")
        print(f"Option token IDs: {self.option_token_ids}")

    # ------------------------------------------------------------------ #
    # 1. 第一次生成（带 ICL 的完整答案）
    # ------------------------------------------------------------------ #
    def generate_answers(self, questions: List[str], sampling_params: SamplingParams) -> List[str]:
        outputs = self.llm.generate(questions, sampling_params)
        sorted_outputs = sorted(outputs, key=lambda x: x.request_id)
        return [output.outputs[0].text.strip() for output in sorted_outputs]

    # ------------------------------------------------------------------ #
    # 2. 第二次生成：让模型“自省”并只输出选项字母
    # ------------------------------------------------------------------ #
    def _build_extract_choice_prompt(self, question: str, generated_text: str) -> str:
        """
        构造一个极简 prompt，要求模型只返回 A/B/C/D（单 token）。
        """
        return f"""You are given a multiple-choice question and a candidate answer generated by yourself.

Question:
{question}

Candidate answer:
{generated_text}

Now, **choose the correct option** by responding with **only one letter** (A, B, C, or D).  
Do **not** write any explanation, box, or other text.

Answer: """

    def extract_choices(self, questions: List[str], generated_texts: List[str]) -> List[str]:
        """
        让模型再看一次自己的生成，只输出选项字母。
        返回形如 ['C', 'A', ...] 的列表。
        """
        prompts = [
            self._build_extract_choice_prompt(q, txt)
            for q, txt in zip(questions, generated_texts)
        ]

        # 关键：max_tokens=1 + logprobs → 直接取最高概率 token
        guided_decoding_params = GuidedDecodingParams(choice=["A", "B", "C", "D", "E"])
        sampling_params = SamplingParams(
            temperature=0.0,      # 确定性
            max_tokens=1,
            logprobs=20
            , guided_decoding=guided_decoding_params
        )

        outputs = self.llm.generate(prompts, sampling_params)
        sorted_outputs = sorted(outputs, key=lambda x: x.request_id)

        extracted = []
        for out in sorted_outputs:
            logprobs_dict = out.outputs[0].logprobs[0]   # 第一个 token 的 logprobs
            # 找出在 self.option_token_ids 中概率最高的 token
            best_token_id = None
            best_logprob = -float('inf')
            for token_id in self.option_token_ids.values():
                lp_obj = logprobs_dict.get(token_id)
                if lp_obj is not None and lp_obj.logprob > best_logprob:
                    best_logprob = lp_obj.logprob
                    best_token_id = token_id

            if best_token_id is not None:
                # 逆向查找字母
                for letter, tid in self.option_token_ids.items():
                    if tid == best_token_id:
                        # 去掉可能的标点，只保留 A/B/C/D
                        extracted.append(letter.strip(" ,、").upper())
                        break
                else:
                    extracted.append("?")
            else:
                extracted.append("?")
        return extracted

    # ------------------------------------------------------------------ #
    # 3. P(True) 计算（保持原样，只是 prompt 中使用 A/B）
    # ------------------------------------------------------------------ #
    def calculate_p_true(self, questions: List[str], answers: List[str]) -> List[float]:
        prompts = [self._build_verification_prompt(q, a) for q, a in zip(questions, answers)]

        sampling_params = SamplingParams(
            temperature=1.0,
            max_tokens=1,
            logprobs=20,
            prompt_logprobs=0,
        )

        outputs = self.llm.generate(prompts, sampling_params)
        sorted_outputs = sorted(outputs, key=lambda x: x.request_id)

        p_true_scores = []
        for output in sorted_outputs:
            if output.outputs[0].logprobs:
                first_token_logprobs = output.outputs[0].logprobs[0]

                yes_lp_obj = first_token_logprobs.get(self.yes_token_id)
                no_lp_obj  = first_token_logprobs.get(self.no_token_id)

                yes_logprob = yes_lp_obj.logprob if yes_lp_obj else -float('inf')
                no_logprob  = no_lp_obj.logprob  if no_lp_obj  else -float('inf')

                yes_prob = np.exp(yes_logprob) if yes_logprob != -float('inf') else 0.0
                no_prob  = np.exp(no_logprob)  if no_logprob  != -float('inf') else 0.0
                total = yes_prob + no_prob
                p_true = yes_prob / total if total > 0 else 0.0
                p_true_scores.append(p_true)
            else:
                p_true_scores.append(0.5)
        return p_true_scores

    def _build_verification_prompt(self, question: str, answer: str) -> str:
        return f"""You are given a question and an answer.

Question:
{question}

We want to verify whether the following answer is correct:
'{answer}'

Please answer the following binary-choice question:

Which of the following is correct?
A: The answer above is correct.
B: The answer above is incorrect.

Response with A or B. No other words or explanation:
Answer: 
"""

    # ------------------------------------------------------------------ #
    # 4. Prompt 构造（ICL 与无 ICL 版）
    # ------------------------------------------------------------------ #
    def format_prompt(self, example, icl=True):
        icl_context = """Please answer the following multiple choice question and put your final answer in \\boxed{{}}.

Example:
Question: What is the capital of France?
A: London
B: Berlin  
C: Paris
D: Madrid

Final answer: \\boxed{{C}}

Now answer this question:

"""
     
        question = example['question']
        label = example['label']
        text = example['choices']

        prompt = f"Question: {question}\n"
        for i in range(len(text)):
            prompt += f"{label[i]}: {text[i]}\n"

        if icl:
            prompt = icl_context + prompt
        return prompt

    # ------------------------------------------------------------------ #
    # 5. 主评估流程
    # ------------------------------------------------------------------ #
    def evaluate_dataset(self, dataset):
        # 1. 构造带 ICL 的问题 → 第一次生成（完整答案）
        questions_with_icl = [self.format_prompt(item, icl=True) for item in dataset]
        generation_params = SamplingParams(
            temperature=0.3,
            top_p=0.9,
            max_tokens=100,
        )
        initial_answers = self.generate_answers(questions_with_icl, generation_params)

        # 2. 让模型自省 → 第二次生成（只输出选项字母）
        print("Step 1: Extracting chosen options via self-reflection...")
        chosen_options = self.extract_choices(
            questions=[self.format_prompt(item, icl=False) for item in dataset],
            generated_texts=initial_answers,
        )

        # 3. 计算 P(True)
        print("Step 2: Calculating P(True) scores...")
        p_true_scores = self.calculate_p_true(
            questions=[self.format_prompt(item, icl=False) for item in dataset],
            answers=initial_answers,
        )

        # 4. 组装结果
        results = {
            "Yhat": chosen_options,          # 现在是可靠的 A/B/C/D
            "Y": [item.get('answer', '') for item in dataset],
            "confidence": p_true_scores,
        }
        save_result(self.args, results)