"""
Plotting Script: P-value Distributions.
Visualizes the separation between Null (H0) and Alternative (H1) hypotheses
under different uncertainty scoring functions using KDE plots.
"""

import argparse
import os
import sys

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
# Assuming these exist in your repo structure
from algorithm.select_alg import get_p_values 

def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--dataset", type=str, default="resnet34_imagenet_scores.csv")
    parser.add_argument("--data_dir", type=str, default="./results") # Changed default to results
    parser.add_argument("--score_function", type=str, default="msp", 
                        choices=["msp", "entropy", "energy", "odin"])
    parser.add_argument("--calib_ratio", type=float, default=0.1)
    parser.add_argument("--alpha", type=float, default=0.1)
    return parser.parse_args()

def main():
    args = parse_args()
    
    # Load Data
    file_path = os.path.join(args.data_dir, args.dataset)
    if not os.path.exists(file_path):
        print(f"Error: File {file_path} not found.")
        return

    df = pd.read_csv(file_path)
    
    # Map score function names to CSV column names
    # Note: Ensure these columns exist in your CSVs (generated by score_comparison.py)
    col_map = {
        "msp": "msp_confidence",
        "entropy": "entropy_confidence",
        "energy": "energy_confidence",
        "odin": "odin_confidence" # Assuming you added ODIN to the score CSV
    }
    
    score_col = col_map[args.score_function]
    if score_col not in df.columns:
        print(f"Error: Column {score_col} not found in dataset. Available: {df.columns}")
        # Fallback for flexibility
        if args.score_function in df.columns:
            score_col = args.score_function
        else:
            return

    Y = df["Y"].to_numpy()
    Yhat = df["Yhat"].to_numpy()
    confidence = df[score_col].to_numpy()
    
    # Setup P-value Calculation
    n_samples = len(Y)
    n_calib = int(n_samples * args.calib_ratio)
    cal_indices = np.random.choice(n_samples, size=n_calib, replace=False)
    
    # Create a dummy args object for get_p_values if it requires one
    dummy_args = argparse.Namespace()
    dummy_args.random = "True" 
    
    print(f"Computing p-values using {args.score_function}...")
    
    # Call core algorithm
    # Note: Ensure get_p_values matches the signature in select_alg.py
    p_values, y_test, y_test_hat, t = get_p_values(
        Y, Yhat, confidence, cal_indices, args.alpha, 
        dummy_args, calib_ratio=args.calib_ratio, random=True
    )
    
    # Separate H0 (Incorrect) and H1 (Correct)
    # H0: Misclassification (We want high p-values ideally, or low? 
    # Usually in Conformal: High Uncertainty -> High P-value -> Fail to reject H0)
    # Check your specific definition. Based on the paper abstract:
    # "Correct labels will receive small p-values" -> Rejection region is small p-values.
    
    # H0: Incorrect Prediction (Should follow Uniform or be large)
    # H1: Correct Prediction (Should be small, clustered near 0)
    
    p_values_h0 = p_values[y_test != y_test_hat] # Incorrect
    p_values_h1 = p_values[y_test == y_test_hat] # Correct

    # Plotting
    plt.figure(figsize=(10, 8))
    sns.set_style("white")
    
    # KDE Plots
    sns.kdeplot(p_values_h0, color='#1f77b4', fill=True, alpha=0.4, linewidth=0, label=r'Null ($H_0$): Incorrect')
    sns.kdeplot(p_values_h1, color='#d62728', fill=True, alpha=0.4, linewidth=0, label=r'Alternative ($H_1$): Correct')
    
    # Outline
    sns.kdeplot(p_values_h0, color='#1f77b4', fill=False, linewidth=2)
    sns.kdeplot(p_values_h1, color='#d62728', fill=False, linewidth=2)

    plt.xlabel("Conformal p-values", fontsize=28)
    plt.ylabel("Density", fontsize=28)
    plt.tick_params(axis='both', which='major', labelsize=20)
    plt.legend(fontsize=20, loc='upper right', frameon=True, shadow=True)
    plt.xlim(0, 1)
    
    out_name = f"p_value_dist_{args.score_function}.pdf"
    plt.tight_layout()
    plt.savefig(out_name, dpi=300)
    print(f"Saved plot to {out_name}")
    plt.show()

if __name__ == "__main__":
    main()
